{"cells":[{"cell_type":"markdown","metadata":{"id":"XBITN0M_LKds"},"source":["# HW2P2: Face Classification and Verification\n"]},{"cell_type":"markdown","metadata":{"id":"-NH4P-HzLRQs"},"source":["Congrats on coming to the second homework in 11785: Introduction to Deep Learning. This homework significantly longer and tougher than the previous homework. You have 2 sub-parts as outlined below. Please start early! \n","\n","\n","*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7000 identities\n","*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities\n","\n","For this HW, you only have to write code to implement your model architecture. Everything else has been provided for you, on the pretext that most of your time will be used up in developing the suitable model architecture for achieving satisfactory performance."]},{"cell_type":"markdown","metadata":{"id":"i1B_m84_cU6c"},"source":["Common errors which you may face in this homeworks (because of the size of the model)\n","\n","\n","*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BdoDIKWOMF59"},"source":["# Preliminaries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1666908241207,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"Jza7lwiScUhb","outputId":"e65d0062-a1d2-461d-de12-30a51e0433a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Oct 27 22:04:00 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   60C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi # to see what GPU you have"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13675,"status":"ok","timestamp":1666908254879,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"bTxfd_nqFnL9","outputId":"9322c1a8-94dc-4f99-c08a-6b519ffedfc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.9 MB 14.0 MB/s \n","\u001b[K     |████████████████████████████████| 182 kB 72.0 MB/s \n","\u001b[K     |████████████████████████████████| 166 kB 56.6 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[K     |████████████████████████████████| 166 kB 61.9 MB/s \n","\u001b[K     |████████████████████████████████| 162 kB 56.8 MB/s \n","\u001b[K     |████████████████████████████████| 162 kB 54.2 MB/s \n","\u001b[K     |████████████████████████████████| 158 kB 49.8 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 58.0 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 55.2 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 28.6 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 52.1 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 58.8 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 62.7 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 23.4 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 55.6 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install wandb --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3482,"status":"ok","timestamp":1666908258343,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"jwLEd0gdPbSc","outputId":"2fab475a-b38b-4983-a592-bfe572682652"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cuda\n","Tesla T4\n","n_cpu = 2\n"]}],"source":["import torch\n","from torchsummary import summary\n","import torchvision #This library is used for image-based operations (Augmentations)\n","import torchvision.transforms as ttf\n","import os\n","import gc\n","from tqdm import tqdm\n","from PIL import Image\n","import multiprocessing\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","import glob\n","import wandb\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)\n","print(torch.cuda.get_device_name(0))\n","n_cpu = multiprocessing.cpu_count()\n","print(f\"n_cpu = {n_cpu}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18412,"status":"ok","timestamp":1666908276739,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"SRz9et3SZnbO","outputId":"acb5b0cd-84a6-4611-907c-bd6ba3575b02"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LdweeXjuUg64"},"outputs":[],"source":["# import os.path as path \n","# if not path.exists(\"/content/drive\"):\n","#   !sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","#   !sudo apt-get update -qq 2>&1 > /dev/null\n","#   !sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n","#   !google-drive-ocamlfuse\n","\n","#   !sudo apt-get install -qq w3m # to act as web browser \n","#   !xdg-settings set default-web-browser w3m.desktop # to set default browser\n","#   %cd /content\n","#   !mkdir drive\n","#   %cd drive\n","#   !mkdir MyDrive\n","#   %cd ..\n","#   %cd ..\n","#   !google-drive-ocamlfuse /content/drive/MyDrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12110,"status":"ok","timestamp":1666908288835,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"Itya1Fu4qkYP","outputId":"41a85baa-ef2e-45c2-9984-c19f0247b3cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["cp: -r not specified; omitting directory '/content/drive/MyDrive/CMU/TA/11785/hw2/hw2p2/models/__pycache__'\n","cp: -r not specified; omitting directory '/content/drive/MyDrive/CMU/TA/11785/hw2/hw2p2/tools/__pycache__'\n","cp: -r not specified; omitting directory '/content/drive/MyDrive/CMU/TA/11785/hw2/hw2p2/trainer/__pycache__'\n"]}],"source":["if not os.path.exists('/content/models'):\n","    os.mkdir(f\"/content/models\")\n","!cp /content/drive/MyDrive/CMU/TA/11785/hw2/hw2p2/models/* /content/models\n","if not os.path.exists('/content/tools'):\n","    os.mkdir(f\"/content/tools\")\n","!cp /content/drive/MyDrive/CMU/TA/11785/hw2/hw2p2/tools/* /content/tools\n","if not os.path.exists('/content/loss'):\n","    os.mkdir(f\"/content/loss\")\n","!cp /content/drive/MyDrive/CMU/TA/11785/hw2/hw2p2/loss/* /content/loss\n","if not os.path.exists('/content/trainer'):\n","    os.mkdir(f\"/content/trainer\")\n","!cp /content/drive/MyDrive/CMU/TA/11785/hw2/hw2p2/trainer/* /content/trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I55eFOO0rQAn"},"outputs":[],"source":["from models import ConvNeXt\n","from loss import CenterLoss\n","from trainer import HW2P2TrainerWithFineTuneLoss\n","from tools.load_model import load_model\n","from tools.kaggle import setup_kaggle"]},{"cell_type":"markdown","metadata":{"id":"1oxQNl-YVWHc"},"source":["# TODOs\n","As you go, please read the code and keep an eye out for TODOs!"]},{"cell_type":"markdown","metadata":{"id":"scOnMklwWBY6"},"source":["# Download Data from Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2668,"status":"ok","timestamp":1666908292041,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"6BksgPdkQwwb","outputId":"d8c4de0e-fb41-45c7-b1ac-f381fd395bfe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting kaggle==1.5.12\n","  Downloading kaggle-1.5.12.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=e24bc5ca2059467361af8b941475398bf18b5cc3fc0a9fbb460f013d2473aee9\n","  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n"]}],"source":["setup_kaggle()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72471,"status":"ok","timestamp":1666908364493,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"3oFjaJTaRjT7","outputId":"0504409d-3273-41cf-c153-f6cc5a5e1a42"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading 11-785-f22-hw2p2-classification.zip to /content\n"," 99% 2.34G/2.37G [00:25<00:00, 109MB/s]\n","100% 2.37G/2.37G [00:25<00:00, 100MB/s]\n","Downloading 11-785-f22-hw2p2-verification.zip to /content\n"," 54% 9.00M/16.8M [00:00<00:00, 29.8MB/s]\n","100% 16.8M/16.8M [00:00<00:00, 47.8MB/s]\n"]}],"source":["!mkdir '/content/data'\n","\n","!kaggle competitions download -c 11-785-f22-hw2p2-classification\n","!unzip -qo '11-785-f22-hw2p2-classification.zip' -d '/content/data'\n","\n","!kaggle competitions download -c 11-785-f22-hw2p2-verification\n","!unzip -qo '11-785-f22-hw2p2-verification.zip' -d '/content/data'"]},{"cell_type":"markdown","metadata":{"id":"O68hT27SXClj"},"source":["# Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aJL29bjQsrIT"},"outputs":[],"source":["import datetime\n","\n","model_name = \"convnext-T-CenterLoss\"\n","checkpoints = '/content/model_checkpoints'\n","if not os.path.exists(checkpoints):\n","    os.mkdir(checkpoints)\n","drive_checkpoint = \"/content/drive/MyDrive/CMU/TA/11785/hw2/hw2p2/model_checkpoints\"\n","if not os.path.exists(drive_checkpoint):\n","    os.mkdir(drive_checkpoint)\n","\n","date = datetime.datetime.now()\n","model_id = model_name + \"_\" + date.strftime(\"%m-%d_%H:%M:%S\")\n","if not os.path.exists(f\"{drive_checkpoint}/{model_id}\"):\n","    os.mkdir(f\"{drive_checkpoint}/{model_id}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7qpMxG0XCJz"},"outputs":[],"source":["\n","\n","config = {\n","    'batch_size': 128, # Increase this if your GPU can handle it\n","    'lr': 0.1,\n","    'epochs': 100,\n","    'weight_decay': 1e-4,\n","    'momentum': 0.9,\n","    'optimizer': 'SGD',\n","    'scheduler': 'CosineAnnealingLR',\n","    'loss_lr': 0.1,\n","    'loss_weight': 0.001, \n","    'label_smoothing': 0.1, \n","    'stochastic_depth_prob': [0, 0.1, 0.2, 0.2], \n","    # 10 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n","    # Include other parameters as needed.\n","}"]},{"cell_type":"markdown","metadata":{"id":"sSeiKHYrM-6b"},"source":["# Classification Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmRX5omaNDEZ"},"outputs":[],"source":["DATA_DIR = '/content/data/11-785-f22-hw2p2-classification/'# TODO: Path where you have downloaded the data\n","TRAIN_DIR = os.path.join(DATA_DIR, \"classification/train\") \n","VAL_DIR = os.path.join(DATA_DIR, \"classification/dev\")\n","TEST_DIR = os.path.join(DATA_DIR, \"classification/test\")\n","\n","# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n","\n","train_transforms = ttf.Compose([ \n","    # Implementing the right transforms/augmentation methods is key to improving performance.\n","                    ttf.RandomApply(transforms=[ttf.RandAugment()], p=0.1), \n","                    ttf.RandomHorizontalFlip(p=0.5), \n","                    ttf.RandomAdjustSharpness(sharpness_factor=2),\n","                    ttf.RandomApply(transforms=[ttf.ColorJitter(brightness=.3, hue=.1, saturation=0.2)], p=0.5), \n","                    ttf.RandomResizedCrop((224, 224), scale=(0.25, 1.0)),\n","                    ttf.RandomApply(transforms=[ttf.RandomRotation(degrees=(-10, 10))], p=0.5), \n","                    ttf.RandomPerspective(distortion_scale=0.4, p=0.75), \n","                    ttf.ToTensor(), \n","                    ])\n","# Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n","# But there are some transforms which are performed after ToTensor() : e.g - Normalization\n","# Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n","\n","val_transforms = ttf.Compose([ttf.ToTensor()])\n","\n","\n","train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, transform = train_transforms)\n","val_dataset = torchvision.datasets.ImageFolder(VAL_DIR, transform = val_transforms)\n","# You should NOT have data augmentation on the validation set. Why?\n","\n","\n","# Create data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = config['batch_size'], \n","                                           shuffle = True, drop_last=True, num_workers = n_cpu, pin_memory = True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = config['batch_size'], \n","                                         shuffle = False, num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SqSR063BGE2e"},"outputs":[],"source":["# You can do this with ImageFolder as well, but it requires some tweaking\n","class ClassificationTestDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_dir, transforms):\n","        self.data_dir   = data_dir\n","        self.transforms = transforms\n","\n","        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n","        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","    \n","    def __getitem__(self, idx):\n","        return self.transforms(Image.open(self.img_paths[idx]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVLB41KtGC2o"},"outputs":[],"source":["test_dataset = ClassificationTestDataset(TEST_DIR, transforms = val_transforms) #Why are we using val_transforms for Test Data?\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n","                         drop_last = False, num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1666908366132,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"x4t8eU9gY0Jy","outputId":"e737e18c-60c1-4284-93ad-d9e9bceb187d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of classes:  7000\n","No. of train images:  140000\n","Shape of image:  torch.Size([3, 224, 224])\n","Batch size:  128\n","Train batches:  1093\n","Val batches:  274\n"]}],"source":["print(\"Number of classes: \", len(train_dataset.classes))\n","print(\"No. of train images: \", train_dataset.__len__())\n","print(\"Shape of image: \", train_dataset[0][0].shape)\n","print(\"Batch size: \", config['batch_size'])\n","print(\"Train batches: \", train_loader.__len__())\n","print(\"Val batches: \", val_loader.__len__())"]},{"cell_type":"markdown","metadata":{"id":"KZCn0qHuZRKj"},"source":["# Setup everything for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIT7NEGHNZbA"},"outputs":[],"source":["# model = FaceClassifier(num_classes=7000, normalize_embeddings=False).to(device)   \n","#model = ResNet50(num_classes=7000).to(device)\n","model = ConvNeXt(stochastic_depth_prob=config['stochastic_depth_prob']).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1666908380330,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"NILtB4O8NZ2n","outputId":"0e3d035e-ce69-4ad4-ee58-8a179aec50f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["33189880\n"]}],"source":["num_trainable_parameters = 0\n","for p in model.parameters():\n","    num_trainable_parameters += p.numel()\n","print(num_trainable_parameters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UowI9OcUYPjP"},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])# TODO: What loss do you need for a multi class classification problem?\n","fine_tuning_loss = CenterLoss(num_classes=7000, feat_dim=768, use_gpu=True)\n","optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=config['weight_decay'])\n","optimizer_loss = torch.optim.SGD(fine_tuning_loss.parameters(), lr = config['loss_lr'])\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * config['epochs']))\n","# TODO: Implement a scheduler (Optional but Highly Recommended)\n","# You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n","scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you\n","# It is useful only in the case of compatible GPUs such as T4/V100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1666908380609,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"akM4YYBT-xNZ","outputId":"8ca4a2f5-a6a5-480f-ab57-4606fe18535f"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 96, 56, 56]           4,704\n","       BatchNorm2d-2           [-1, 96, 56, 56]             192\n","            Conv2d-3           [-1, 96, 56, 56]           4,704\n","       BatchNorm2d-4           [-1, 96, 56, 56]             192\n","            Conv2d-5          [-1, 384, 56, 56]          37,248\n","              GELU-6          [-1, 384, 56, 56]               0\n","            Conv2d-7           [-1, 96, 56, 56]          36,960\n","   StochasticDepth-8           [-1, 96, 56, 56]               0\n","             Block-9           [-1, 96, 56, 56]               0\n","           Conv2d-10           [-1, 96, 56, 56]           4,704\n","      BatchNorm2d-11           [-1, 96, 56, 56]             192\n","           Conv2d-12          [-1, 384, 56, 56]          37,248\n","             GELU-13          [-1, 384, 56, 56]               0\n","           Conv2d-14           [-1, 96, 56, 56]          36,960\n","  StochasticDepth-15           [-1, 96, 56, 56]               0\n","            Block-16           [-1, 96, 56, 56]               0\n","           Conv2d-17           [-1, 96, 56, 56]           4,704\n","      BatchNorm2d-18           [-1, 96, 56, 56]             192\n","           Conv2d-19          [-1, 384, 56, 56]          37,248\n","             GELU-20          [-1, 384, 56, 56]               0\n","           Conv2d-21           [-1, 96, 56, 56]          36,960\n","  StochasticDepth-22           [-1, 96, 56, 56]               0\n","            Block-23           [-1, 96, 56, 56]               0\n","      BatchNorm2d-24           [-1, 96, 56, 56]             192\n","           Conv2d-25          [-1, 192, 28, 28]          73,920\n","           Conv2d-26          [-1, 192, 28, 28]           9,408\n","      BatchNorm2d-27          [-1, 192, 28, 28]             384\n","           Conv2d-28          [-1, 768, 28, 28]         148,224\n","             GELU-29          [-1, 768, 28, 28]               0\n","           Conv2d-30          [-1, 192, 28, 28]         147,648\n","  StochasticDepth-31          [-1, 192, 28, 28]               0\n","            Block-32          [-1, 192, 28, 28]               0\n","           Conv2d-33          [-1, 192, 28, 28]           9,408\n","      BatchNorm2d-34          [-1, 192, 28, 28]             384\n","           Conv2d-35          [-1, 768, 28, 28]         148,224\n","             GELU-36          [-1, 768, 28, 28]               0\n","           Conv2d-37          [-1, 192, 28, 28]         147,648\n","  StochasticDepth-38          [-1, 192, 28, 28]               0\n","            Block-39          [-1, 192, 28, 28]               0\n","           Conv2d-40          [-1, 192, 28, 28]           9,408\n","      BatchNorm2d-41          [-1, 192, 28, 28]             384\n","           Conv2d-42          [-1, 768, 28, 28]         148,224\n","             GELU-43          [-1, 768, 28, 28]               0\n","           Conv2d-44          [-1, 192, 28, 28]         147,648\n","  StochasticDepth-45          [-1, 192, 28, 28]               0\n","            Block-46          [-1, 192, 28, 28]               0\n","      BatchNorm2d-47          [-1, 192, 28, 28]             384\n","           Conv2d-48          [-1, 384, 14, 14]         295,296\n","           Conv2d-49          [-1, 384, 14, 14]          18,816\n","      BatchNorm2d-50          [-1, 384, 14, 14]             768\n","           Conv2d-51         [-1, 1536, 14, 14]         591,360\n","             GELU-52         [-1, 1536, 14, 14]               0\n","           Conv2d-53          [-1, 384, 14, 14]         590,208\n","  StochasticDepth-54          [-1, 384, 14, 14]               0\n","            Block-55          [-1, 384, 14, 14]               0\n","           Conv2d-56          [-1, 384, 14, 14]          18,816\n","      BatchNorm2d-57          [-1, 384, 14, 14]             768\n","           Conv2d-58         [-1, 1536, 14, 14]         591,360\n","             GELU-59         [-1, 1536, 14, 14]               0\n","           Conv2d-60          [-1, 384, 14, 14]         590,208\n","  StochasticDepth-61          [-1, 384, 14, 14]               0\n","            Block-62          [-1, 384, 14, 14]               0\n","           Conv2d-63          [-1, 384, 14, 14]          18,816\n","      BatchNorm2d-64          [-1, 384, 14, 14]             768\n","           Conv2d-65         [-1, 1536, 14, 14]         591,360\n","             GELU-66         [-1, 1536, 14, 14]               0\n","           Conv2d-67          [-1, 384, 14, 14]         590,208\n","  StochasticDepth-68          [-1, 384, 14, 14]               0\n","            Block-69          [-1, 384, 14, 14]               0\n","           Conv2d-70          [-1, 384, 14, 14]          18,816\n","      BatchNorm2d-71          [-1, 384, 14, 14]             768\n","           Conv2d-72         [-1, 1536, 14, 14]         591,360\n","             GELU-73         [-1, 1536, 14, 14]               0\n","           Conv2d-74          [-1, 384, 14, 14]         590,208\n","  StochasticDepth-75          [-1, 384, 14, 14]               0\n","            Block-76          [-1, 384, 14, 14]               0\n","           Conv2d-77          [-1, 384, 14, 14]          18,816\n","      BatchNorm2d-78          [-1, 384, 14, 14]             768\n","           Conv2d-79         [-1, 1536, 14, 14]         591,360\n","             GELU-80         [-1, 1536, 14, 14]               0\n","           Conv2d-81          [-1, 384, 14, 14]         590,208\n","  StochasticDepth-82          [-1, 384, 14, 14]               0\n","            Block-83          [-1, 384, 14, 14]               0\n","           Conv2d-84          [-1, 384, 14, 14]          18,816\n","      BatchNorm2d-85          [-1, 384, 14, 14]             768\n","           Conv2d-86         [-1, 1536, 14, 14]         591,360\n","             GELU-87         [-1, 1536, 14, 14]               0\n","           Conv2d-88          [-1, 384, 14, 14]         590,208\n","  StochasticDepth-89          [-1, 384, 14, 14]               0\n","            Block-90          [-1, 384, 14, 14]               0\n","           Conv2d-91          [-1, 384, 14, 14]          18,816\n","      BatchNorm2d-92          [-1, 384, 14, 14]             768\n","           Conv2d-93         [-1, 1536, 14, 14]         591,360\n","             GELU-94         [-1, 1536, 14, 14]               0\n","           Conv2d-95          [-1, 384, 14, 14]         590,208\n","  StochasticDepth-96          [-1, 384, 14, 14]               0\n","            Block-97          [-1, 384, 14, 14]               0\n","           Conv2d-98          [-1, 384, 14, 14]          18,816\n","      BatchNorm2d-99          [-1, 384, 14, 14]             768\n","          Conv2d-100         [-1, 1536, 14, 14]         591,360\n","            GELU-101         [-1, 1536, 14, 14]               0\n","          Conv2d-102          [-1, 384, 14, 14]         590,208\n"," StochasticDepth-103          [-1, 384, 14, 14]               0\n","           Block-104          [-1, 384, 14, 14]               0\n","          Conv2d-105          [-1, 384, 14, 14]          18,816\n","     BatchNorm2d-106          [-1, 384, 14, 14]             768\n","          Conv2d-107         [-1, 1536, 14, 14]         591,360\n","            GELU-108         [-1, 1536, 14, 14]               0\n","          Conv2d-109          [-1, 384, 14, 14]         590,208\n"," StochasticDepth-110          [-1, 384, 14, 14]               0\n","           Block-111          [-1, 384, 14, 14]               0\n","     BatchNorm2d-112          [-1, 384, 14, 14]             768\n","          Conv2d-113            [-1, 768, 7, 7]       1,180,416\n","          Conv2d-114            [-1, 768, 7, 7]          37,632\n","     BatchNorm2d-115            [-1, 768, 7, 7]           1,536\n","          Conv2d-116           [-1, 3072, 7, 7]       2,362,368\n","            GELU-117           [-1, 3072, 7, 7]               0\n","          Conv2d-118            [-1, 768, 7, 7]       2,360,064\n"," StochasticDepth-119            [-1, 768, 7, 7]               0\n","           Block-120            [-1, 768, 7, 7]               0\n","          Conv2d-121            [-1, 768, 7, 7]          37,632\n","     BatchNorm2d-122            [-1, 768, 7, 7]           1,536\n","          Conv2d-123           [-1, 3072, 7, 7]       2,362,368\n","            GELU-124           [-1, 3072, 7, 7]               0\n","          Conv2d-125            [-1, 768, 7, 7]       2,360,064\n"," StochasticDepth-126            [-1, 768, 7, 7]               0\n","           Block-127            [-1, 768, 7, 7]               0\n","          Conv2d-128            [-1, 768, 7, 7]          37,632\n","     BatchNorm2d-129            [-1, 768, 7, 7]           1,536\n","          Conv2d-130           [-1, 3072, 7, 7]       2,362,368\n","            GELU-131           [-1, 3072, 7, 7]               0\n","          Conv2d-132            [-1, 768, 7, 7]       2,360,064\n"," StochasticDepth-133            [-1, 768, 7, 7]               0\n","           Block-134            [-1, 768, 7, 7]               0\n","       AvgPool2d-135            [-1, 768, 1, 1]               0\n","     BatchNorm2d-136            [-1, 768, 1, 1]           1,536\n","         Flatten-137                  [-1, 768]               0\n","         Dropout-138                  [-1, 768]               0\n","          Linear-139                 [-1, 7000]       5,383,000\n","================================================================\n","Total params: 33,189,880\n","Trainable params: 33,189,880\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 223.45\n","Params size (MB): 126.61\n","Estimated Total Size (MB): 350.63\n","----------------------------------------------------------------\n"]}],"source":["summary(model, input_size=(3, 224, 224))"]},{"cell_type":"markdown","metadata":{"id":"dzM11HtcboYv"},"source":["# Let's train!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0Q31vHVwviw"},"outputs":[],"source":["trainer = HW2P2TrainerWithFineTuneLoss(\n","    model_id, \n","    config['batch_size'], \n","    device, \n","    checkpoints, \n","    max_epochs = config['epochs'], \n","    epoch_start = 0, \n","    log_checkpoints = drive_checkpoint,\n","    model_saver_mode = 'max'\n",")\n","trainer.save_spec(str(model))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmotca6pcLLY"},"outputs":[],"source":["gc.collect() # These commands help you when you face CUDA OOM error\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"2mBgKGkXLrdJ"},"source":["# Wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2567,"status":"ok","timestamp":1666908383378,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"Ix62_BkaLr_D","outputId":"05a3afb4-f39d-4d92-fbab-c820a90e9c68"},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key=\"<wandb-key>\") #API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"elapsed":1619,"status":"ok","timestamp":1666908384975,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"VG0vmsmbRYEi","outputId":"16a218fd-9cd0-4c80-e853-c5c7db10080e"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mghensk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20221027_220623-1rfgbux9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/ghensk/hw2p2-ablations/runs/1rfgbux9\" target=\"_blank\">convnext-T-CenterLoss_10-27_22:06:04</a></strong> to <a href=\"https://wandb.ai/ghensk/hw2p2-ablations\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = model_id, ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # run_id = ### Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw2p2-ablations\", ### Project should be created in your wandb account \n","    config = config, ### Wandb Config for your run\n","    group = 'ConvNeXt'\n",")"]},{"cell_type":"markdown","metadata":{"id":"PsJx1l1T4twC"},"source":["# Verification Task Setup"]},{"cell_type":"markdown","metadata":{"id":"FoBFFF8-Lpvj"},"source":["The verification task consists of the following generalized scenario:\n","- You are given X unknown identitites \n","- You are given Y known identitites\n","- Your goal is to match X unknown identities to Y known identities.\n","\n","We have given you a verification dataset, that consists of 1000 known identities, and 1000 unknown identities. The 1000 unknown identities are split into dev (200) and test (800). Your goal is to compare the unknown identities to the 1000 known identities and assign an identity to each image from the set of unknown identities. \n","\n","Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities. \n","\n","This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3466,"status":"ok","timestamp":1666908388424,"user":{"displayName":"TAKATOMO SAITO","userId":"14836162415061614130"},"user_tz":240},"id":"f9aY5o-suWdn","outputId":"0e75927b-79c1-4913-bef1-74a63a567518"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 200/200 [00:00<00:00, 7547.38it/s]\n","100%|██████████| 1000/1000 [00:00<00:00, 10822.19it/s]\n"]}],"source":["known_regex = \"/content/data/verification/known/*/*\"\n","known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))] \n","# This obtains the list of known identities from the known folder\n","\n","unknown_regex = \"/content/data/verification/unknown_dev/*\" #Change the directory accordingly for the test set\n","\n","# We load the images from known and unknown folders\n","unknown_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_regex)))]\n","known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n","\n","# Why do you need only ToTensor() here?\n","transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor()])\n","\n","unknown_images = torch.stack([transforms(x) for x in unknown_images])\n","known_images  = torch.stack([transforms(y) for y in known_images ])\n","#Print your shapes here to understand what we have done\n","\n","# You can use other similarity metrics like Euclidean Distance if you wish\n","similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6) "]},{"cell_type":"markdown","metadata":{"id":"SQkRw1FvLqYe"},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"RPZss5_Rx-uT","outputId":"15c3ccf7-ff08-412a-e26e-f4b16acf1672"},"outputs":[{"name":"stderr","output_type":"stream","text":["Train Epoch 53/100::  95%|█████████▌| 1043/1093 [17:59<00:53,  1.06s/it, acc=0.0172%, fine_tune_loss=0.0290, lr=0.0454, num_correct=23]"]}],"source":["trainer.fit(model, train_loader, val_loader, optimizer, criterion, scheduler, scaler, fine_tuning_loss, optimizer_loss, config['loss_weight'], known_paths, unknown_images, known_images, similarity_metric, wandb, run)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4iE-1WCyk13"},"outputs":[],"source":["import shutil\n","for file in os.listdir(f\"{checkpoints}/{model_id}\"):\n","    src = f\"{checkpoints}/{model_id}/\" + file\n","    dest = f\"{drive_checkpoint}/{model_id}/\" + file\n","    shutil.copy(src, dest)"]},{"cell_type":"markdown","metadata":{"id":"tlRda1Q9OHV-"},"source":[]},{"cell_type":"markdown","metadata":{"id":"UpgCHImRkYQW"},"source":["# Classification Task: Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCIkdWktiBwE"},"outputs":[],"source":["#Load your best model here if you need to\n","# checkpoint = torch.load('checkpoint.pth')\n","# model.load_state_dict(checkpoint['model_state_dict'])\n","# model = trainer.load_best_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5DlbDyCECaT"},"outputs":[],"source":["data = torch.load(f\"{checkpoints}/{model_id}/epoch_98.model\", map_location=device)\n","model = data.model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhbndXQ5hwxG"},"outputs":[],"source":["# val_acc, val_loss = validate(model, val_loader, criterion)\n","# print(\"\\nVal Acc {:.04f} Val Loss {:.04f}\".format(val_acc, val_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2WQEUjXkWvo"},"outputs":[],"source":["def test(model,dataloader):\n","\n","  model.eval()\n","  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n","  test_results = []\n","  \n","  for i, (images) in enumerate(dataloader):\n","      images = images.to(device)\n","\n","      with torch.inference_mode():\n","        outputs = model(images)\n","\n","      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n","      test_results.extend(outputs)\n","      \n","      batch_bar.update()\n","      \n","  batch_bar.close()\n","  return test_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7R1lcCAzULc"},"outputs":[],"source":["test_results = test(model, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"zqfUzwS2L1gx"},"source":["## Generate csv to submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vob9a2-HkW_V"},"outputs":[],"source":["with open(\"classification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(test_dataset)):\n","        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOYlkv_-0Uzr"},"outputs":[],"source":["src = \"classification_early_submission.csv\"\n","dest = f\"{drive_checkpoint}/{model_id}/\" + \"classification_early_submission.csv\"\n","shutil.copy(src, dest)"]},{"cell_type":"markdown","metadata":{"id":"HkRRwSsyOBSZ"},"source":["# Verification: Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_0SvzUTHDj9"},"outputs":[],"source":["unknown_regex = \"/content/data/verification/unknown_test/*\" #Change the directory accordingly for the test set\n","\n","# We load the images from known and unknown folders\n","unknown_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_regex)))]\n","unknown_images = torch.stack([transforms(x) for x in unknown_images])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fO_b2TiN8Ih"},"outputs":[],"source":["pred_id_strings = trainer.eval_verification(model, known_paths, unknown_images, known_images, similarity_metric, config['batch_size'], mode='test')\n","\n","with open(\"verification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(pred_id_strings)):\n","        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwjIpjBN0p-q"},"outputs":[],"source":["src = \"verification_early_submission.csv\"\n","dest = f\"{drive_checkpoint}/{model_id}/\" + \"verification_early_submission.csv\"\n","shutil.copy(src, dest)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["scOnMklwWBY6","sSeiKHYrM-6b","mIqmojPaWD0H","KCA-Wr3bFIxj","KZCn0qHuZRKj","dzM11HtcboYv","2mBgKGkXLrdJ","SQkRw1FvLqYe","zqfUzwS2L1gx"],"provenance":[{"file_id":"1AThuaGPxRB23-thy5l17sJABnVu5i_nF","timestamp":1664639008229},{"file_id":"1S4OmDfJclgVuE-eFHVGcY_sdyA20MtMq","timestamp":1664557363909}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
